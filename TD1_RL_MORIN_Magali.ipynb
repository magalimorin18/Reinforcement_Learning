{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12b03666",
   "metadata": {},
   "source": [
    "## TD 1- Reinforcement Learning - Magali Morin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6218930",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "All modules are installed on my env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757da87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.envs.toy_text.frozen_lake as fl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990a2ab",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "\n",
    "Description of the MDP : \n",
    "\n",
    "States : [0;15]\n",
    "\n",
    "Action : 0 LEFT , 1 DOWN , 2 RIGHT, 3 UP\n",
    "\n",
    "Rewards : 0 for states 0 to 14 and 1 for state 15\n",
    "\n",
    "Transitions P : When choosing the action x âˆˆ [0,3] there is a probability 1/3 to move in the direction x, a probability 1/3 to move in the direction x+1[4] and a probability 1/3 to move in the direction x-1[4]. Furthermore, you can't move towards the boundaries of the lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cf58f",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Choose right policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57cc14",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Monte Carlo estimation of the value fonciton\n",
    "\n",
    "In the Monte Carlo estimation, we update the state function at the end of each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b83b1679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = fl.FrozenLakeEnv()\n",
    "env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daac1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "NBR_EPISODES = 100000\n",
    "HORIZON = 200\n",
    "GAMMA = 0.9\n",
    "SEED = env.seed(42)\n",
    "\n",
    "def expected_value_method(policy): \n",
    "    \"\"\"\n",
    "    policy : array of len nb of states which gives the action to do for each state\n",
    "    return : an value vector with the value to do an action for each state\n",
    "    \"\"\"\n",
    "    value_fnct = np.zeros(env.nS)\n",
    "    for state in range(env.nS): \n",
    "        state_value= 0\n",
    "        env.isd = np.zeros(env.nS)\n",
    "        env.isd[state] = 1\n",
    "        env.reset()\n",
    "        for i in range(NBR_EPISODES) : \n",
    "            time = 0\n",
    "            env.reset()\n",
    "            done = False\n",
    "            discount = 1\n",
    "            while (not done) and (time<HORIZON) : \n",
    "                time += 1\n",
    "                current_state, reward, done, info = env.step(policy[state])\n",
    "                state_value += discount * reward\n",
    "                discount = discount * GAMMA\n",
    "        value_fnct[state] = state_value / NBR_EPISODES\n",
    "    return(value_fnct)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ec9daa",
   "metadata": {},
   "source": [
    "### Question 5 : Linear system method\n",
    "\n",
    "The equation for the linear system method is : $$V^{\\pi} = r_{\\pi}(I-\\gamma P)^{-1}$$\n",
    "\n",
    "with P : the dynamic transition probabilities matrix. P[a][b] is the probability to pass from state a to state b.\n",
    "\n",
    "     I : Identity vector \n",
    "     \n",
    "     R : reward vecor R[a] is the reward given for being is state a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4bcdeeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_policy = np.array([fl.RIGHT for _ in range(env.nS)])\n",
    "\n",
    "def linear_system_method(policy):\n",
    "    \"\"\"\n",
    "    policy : array of len nb of states which gives the action to do for each state\n",
    "    return : an value vector with the value to do an action for each state\n",
    "    \"\"\"\n",
    "    #Create the matrix \n",
    "    I = np.identity(env.nS)\n",
    "    P = np.zeros((env.nS, env.nS))\n",
    "    R = np.zeros(env.nS)\n",
    "    \n",
    "    for state in range(env.nS):\n",
    "        transitions = env.P[state][policy[state]]\n",
    "        for transition in transitions : \n",
    "            proba = transition[0]\n",
    "            next_state = transition[1]\n",
    "            reward = transition[2]\n",
    "            P[state, next_state] += proba\n",
    "            R[state] += reward*proba\n",
    "    value_fnct = np.dot(np.linalg.inv(I-GAMMA*P),R)\n",
    "    return(value_fnct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffc503",
   "metadata": {},
   "source": [
    "We can compare the result of the two policies by calculating the value function in each case for the right policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f299be5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.30776757e-02 1.17595819e-02 2.74390244e-02 1.15266041e-16\n",
      " 1.87549947e-02 1.09819425e-16 6.40243902e-02 1.53688055e-16\n",
      " 4.94389734e-02 1.46041583e-01 1.85975610e-01 0.00000000e+00\n",
      " 0.00000000e+00 3.00829668e-01 5.55894309e-01 0.00000000e+00]\n",
      "[0.01312729 0.01141018 0.026981   0.         0.01894528 0.\n",
      " 0.06315667 0.         0.04905123 0.14661202 0.18430732 0.\n",
      " 0.         0.30034573 0.55512608 0.        ]\n",
      "0.0002608123110508956\n"
     ]
    }
   ],
   "source": [
    "right_policy = np.array([fl.RIGHT for i in range(env.nS)])\n",
    "\n",
    "linear  = linear_system_method(right_policy)\n",
    "monte_carlo = expected_value_method(right_policy)\n",
    "print(linear)\n",
    "print(monte_carlo)\n",
    "\n",
    "delta  = (linear-monte_carlo).mean()\n",
    "\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f03d6c6",
   "metadata": {},
   "source": [
    "We observe that the difference is very small. The big difference is the time of calculation of both algorithms. Linear method is much faster than Monte Carlo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41146f8b",
   "metadata": {},
   "source": [
    "### Question 6 : Bellman operator method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1597ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_operator(policy):\n",
    "    \"\"\"\n",
    "    policy : array of len nb of states which gives the action to do for each state\n",
    "    return : the bellman operator to apply to a value function\n",
    "    \"\"\"\n",
    "    P = np.zeros((env.nS, env.nS))\n",
    "    R = np.zeros(env.nS)\n",
    "    \n",
    "    for state in range(env.nS):\n",
    "        transitions = env.P[state][policy[state]]\n",
    "        for transition in transitions : \n",
    "            proba = transition[0]\n",
    "            next_state = transition[1]\n",
    "            reward = transition[2]\n",
    "            P[state, next_state] += proba\n",
    "            R[state] += reward*proba\n",
    "    return(lambda value : R + GAMMA*np.dot(P,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "85ea24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_method(policy, epsilon):\n",
    "    \"\"\"\n",
    "    policy : array of len nb of states which gives the action to do for each state\n",
    "    epsilon : stopping criteria, normative difference of two consecutive value function\n",
    "    return : the value function for a given policy\n",
    "    \"\"\"\n",
    "    value = np.zeros(env.nS)\n",
    "    next_value = bellman_operator(policy)(value)\n",
    "    \n",
    "    while np.linalg.norm(value - next_value) > epsilon : \n",
    "        value = next_value\n",
    "        next_value = bellman_operator(policy)(value)\n",
    "    \n",
    "    return(next_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7b1afe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01306594 0.01175684 0.02743628 0.         0.01874559 0.\n",
      " 0.06402165 0.         0.04943158 0.14603605 0.18597287 0.\n",
      " 0.         0.30082135 0.55589157 0.        ]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-5\n",
    "print(bellman_method(right_policy, epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f24733",
   "metadata": {},
   "source": [
    "The stopping criterio used was the threshold. We could also add an maximal number of iteration to be sure the loop stops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efad406",
   "metadata": {},
   "source": [
    "### Question 7 : optimal bellman operator\n",
    "\n",
    "\n",
    "Mathematical definition:\n",
    "\n",
    "$$ \\mathcal{T}[v] = \\max_{a} [R_a + \\gamma P_av]$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69756408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_optimal_operator(value, R, P):\n",
    "    \"\"\"\n",
    "    value : value function\n",
    "    R : reward vector\n",
    "    P : dynamic transition probabilities\n",
    "    Returns : optimal bellman operator\n",
    "    \"\"\"\n",
    "    next_value = np.zeros(env.nS)\n",
    "    for state in range(env.nS) : \n",
    "        best_s = 0\n",
    "        for a in range(env.nA) : \n",
    "            value_a = R[state][a] + GAMMA * np.dot(P[:,:,a],value)[state]\n",
    "            if value_a > best_s : \n",
    "                best_s = value_a\n",
    "        next_value[state] = best_s\n",
    "    return(next_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2e6f2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_optimal_method(epsilon):\n",
    "    \"\"\"\n",
    "    epsilon : stopping criteria, normative difference of two consecutive value function\n",
    "    return : an approximation of the value function\n",
    "    \"\"\"\n",
    "    \n",
    "    #We calculate the transition matrix and the reward vector \n",
    "    R = np.zeros((env.nS, env.nA))\n",
    "    P = np.zeros((env.nS,env.nS,env.nA))\n",
    "\n",
    "    for state in range(env.nS):\n",
    "        for action in range(env.nA):\n",
    "            transitions = env.P[state][action]\n",
    "            for transition in transitions : \n",
    "                proba = transition[0]\n",
    "                next_state = transition[1]\n",
    "                reward = transition[2]\n",
    "                P[state, next_state, action] += proba\n",
    "                R[state, action] += reward*proba\n",
    "    \n",
    "    value = np.zeros(env.nS)\n",
    "    next_value = bellman_optimal_operator(value,R,P)\n",
    "    while np.linalg.norm(value - next_value) > epsilon : \n",
    "        value = next_value\n",
    "        next_value = bellman_optimal_operator(value,R,P)\n",
    "    \n",
    "    return(next_value)                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d43cb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06885897 0.06138751 0.07439003 0.05578562 0.0918255  0.\n",
      " 0.11219759 0.         0.14541285 0.24748111 0.29960644 0.\n",
      " 0.         0.37992446 0.63901416 0.        ]\n"
     ]
    }
   ],
   "source": [
    "epsilon = 1e-5\n",
    "print(bellman_optimal_method(epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2122d",
   "metadata": {},
   "source": [
    "### Question 8 : Value iteration method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478401e4",
   "metadata": {},
   "source": [
    "Mathematical definition\n",
    "\n",
    "$$\\mathcal{G}[v] = argmax_{a} [R_a + \\gamma P_av]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6cbedc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_optimal_policy(value,R,P):\n",
    "    \"\"\"\n",
    "    value : value function\n",
    "    R : reward vector\n",
    "    P : dynamic transition probabilities\n",
    "    returns : Bellman policy\n",
    "    \"\"\"\n",
    "    policy = np.zeros(env.nS)\n",
    "    for state in range(env.nS) : \n",
    "        best_s = 0\n",
    "        best_a = 0\n",
    "        for a in range(env.nA) : \n",
    "            value_a = R[state][a] + GAMMA * np.dot(P[:,:,a],value)[state]\n",
    "            if value_a > best_s : \n",
    "                best_s = value_a\n",
    "                best_a = a\n",
    "        policy[state] = best_a\n",
    "    return(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b8f0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_method(epsilon): \n",
    "    \"\"\"\n",
    "    epsilon : stopping criteria, normative difference of two consecutive value function\n",
    "    return : an approximation of the optimal policy\n",
    "    \"\"\"\n",
    "    \n",
    "    #We calculate the transition matrix and the reward vector \n",
    "    R = np.zeros((env.nS, env.nA))\n",
    "    P = np.zeros((env.nS,env.nS,env.nA))\n",
    "\n",
    "    for state in range(env.nS):\n",
    "        for action in range(env.nA):\n",
    "            transitions = env.P[state][action]\n",
    "            for transition in transitions : \n",
    "                proba = transition[0]\n",
    "                next_state = transition[1]\n",
    "                reward = transition[2]\n",
    "                P[state, next_state, action] += proba\n",
    "                R[state, action] += reward*proba\n",
    "    \n",
    "    value = bellman_optimal_method(epsilon)\n",
    "    policy = bellman_optimal_policy(value,R,P)\n",
    "    \n",
    "    return(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "225addaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 3., 0., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_iteration_method(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a8a66",
   "metadata": {},
   "source": [
    "### Question 9 : Policy iteration method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a589f164",
   "metadata": {},
   "source": [
    "Mathematical demonstration : \n",
    "\n",
    "$\\pi_0$ initialized arbitrarily.\n",
    "\n",
    "$v_{n+1} = (I-\\gamma P_{\\pi_n})^{-1}R_{\\pi_n}$.\n",
    "\n",
    "$\\pi_{n+1} = \\mathcal{G}[v_n]$.\n",
    "\n",
    "Stoping criteria: $|v_{n+1} - v_{n}| < \\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2d3e49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration_method(epsilon):\n",
    "    '''\n",
    "    epsilon : stopping criteria, normative difference of two consecutive value function\n",
    "    Return : optimal policy\n",
    "\n",
    "    '''\n",
    "    policy = np.random.randint(0,env.nA,env.nS)\n",
    "    value = linear_system_method(policy) #or bellman method\n",
    "    \n",
    "    #We calculate R and P for this environment\n",
    "    R = np.zeros((env.nS, env.nA))\n",
    "    P = np.zeros((env.nS,env.nS,env.nA))\n",
    "\n",
    "    for state in range(env.nS):\n",
    "        for action in range(env.nA):\n",
    "            transitions = env.P[state][action]\n",
    "            for transition in transitions : \n",
    "                proba = transition[0]\n",
    "                next_state = transition[1]\n",
    "                reward = transition[2]\n",
    "                P[state, next_state, action] += proba\n",
    "                R[state, action] += reward*proba\n",
    "    \n",
    "    next_policy = bellman_optimal_policy(value,R,P)\n",
    "    next_value = linear_system_method(policy)\n",
    "    while np.linalg.norm(value - next_value) > epsilon :\n",
    "        policy = next_policy\n",
    "        value = next_value\n",
    "        next_policy = bellman_optimal_policy(value,R,P)\n",
    "        next_value = linear_system_method(next_policy)\n",
    "    \n",
    "    return(next_policy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "33dd0ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 3., 0., 3., 0., 0., 2., 0., 3., 1., 0., 0., 0., 2., 1., 0.])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon = 1e-5\n",
    "policy_iteration_method(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78559a3f",
   "metadata": {},
   "source": [
    "### Question 10 : Comparison of both methods\n",
    "\n",
    "The two methods are giving similar results.\n",
    "Tu push the comparison further, we could compute the time to see which method is the quickest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974db99",
   "metadata": {},
   "source": [
    "### Question 11 : \n",
    "Using the contraction property of the optimal Bellman operator, implement a function\n",
    "that iteratively applies the optimal Bellman operator to compute the optimal value function\n",
    "Q*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b8d7d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_operator(value,R,P):\n",
    "    next_q = np.zeros((env.nS,env.nA))\n",
    "    for state in range(env.nS):\n",
    "        for action in range(env.nA):\n",
    "            reward = R[state,action]\n",
    "            for state_bis in range(env.nS):\n",
    "                optim_value = 0\n",
    "                for action_bis in range(env.nA):\n",
    "                    value_a = value[state_bis,action_bis]\n",
    "                    if value_a > optim_value:\n",
    "                        optim_value = value_a\n",
    "                reward += GAMMA * P[state,state_bis,action] * optim_value\n",
    "            next_q[state,action] = reward\n",
    "    return(next_q)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1d9781d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_method(epsilon):\n",
    "    \"\"\"\n",
    "    epsilon:stopping criteria\n",
    "    Returns optimal quality function\n",
    "    \"\"\"\n",
    "    #We calculate R and P for this environment\n",
    "    R = np.zeros((env.nS, env.nA))\n",
    "    P = np.zeros((env.nS,env.nS,env.nA))\n",
    "\n",
    "    for state in range(env.nS):\n",
    "        for action in range(env.nA):\n",
    "            transitions = env.P[state][action]\n",
    "            for transition in transitions : \n",
    "                proba = transition[0]\n",
    "                next_state = transition[1]\n",
    "                reward = transition[2]\n",
    "                P[state, next_state, action] += proba\n",
    "                R[state, action] += reward*proba\n",
    "                \n",
    "    q = np.zeros((env.nS,env.nA))\n",
    "    next_q = q_operator(q, R, P)\n",
    "    while np.linalg.norm(q-next_q) > epsilon:\n",
    "        q = next_q\n",
    "        next_q = q_operator(q,R,P)\n",
    "        \n",
    "    return(next_q)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9670c323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06887237, 0.06663045, 0.06663045, 0.05974078],\n",
       "       [0.03907988, 0.04297989, 0.04073797, 0.06139887],\n",
       "       [0.07439831, 0.06881719, 0.0727172 , 0.05747583],\n",
       "       [0.03905686, 0.03905686, 0.03347574, 0.05579473],\n",
       "       [0.09183769, 0.07117679, 0.06428712, 0.04821147],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.11220205, 0.08988305, 0.11220205, 0.02231899],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.07117679, 0.11787214, 0.1017965 , 0.14542271],\n",
       "       [0.15760471, 0.24748776, 0.20386154, 0.13350927],\n",
       "       [0.29961112, 0.26595078, 0.22536519, 0.10790627],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.18822442, 0.30568334, 0.37992926, 0.26595078],\n",
       "       [0.39556639, 0.63901667, 0.61492124, 0.53719488],\n",
       "       [0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_method(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7435e2e9",
   "metadata": {},
   "source": [
    "### Question 12 : Optimal policy for Q\n",
    "\n",
    "$$\\pi* = argmax_{a}[q(s,a)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c8ec7249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_q(epsilon):\n",
    "    q = q_method(epsilon)\n",
    "    p = np.zeros(env.nS)\n",
    "    for state in range(env.nS):\n",
    "        best_q = 0\n",
    "        best_a = 0\n",
    "        for action in range(env.nA):\n",
    "            if q[state,action] > best_q:\n",
    "                best_q = q[state,action]\n",
    "                best_a = action\n",
    "        p[state] = best_a\n",
    "    return(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "16cede92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 3., 0., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_policy_q(epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa4adc5",
   "metadata": {},
   "source": [
    "The results are similar to question 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee448ecd",
   "metadata": {},
   "source": [
    "### Question 13 : Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f550c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def final_trajectory(policy):\n",
    "    \"\"\"\n",
    "    policy : a given policy\n",
    "    Return : trajectory\n",
    "    \"\"\"\n",
    "   \n",
    "    env.reset()\n",
    "    done = False\n",
    "    t = 0\n",
    "    rewards=0\n",
    "    discount = 1\n",
    "    while (not done) and (t < HORIZON):\n",
    "        next_state, r, done, _ = env.step(fl.RIGHT)\n",
    "        rewards += discount * r\n",
    "        discount *= GAMMA\n",
    "        t += 1\n",
    "    if done and env.nS != env.nS-1: \n",
    "        return('Too bad, you lost')\n",
    "    elif done : \n",
    "        return('You won and your reward is ', rewards)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f04f97d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Too bad, you lost'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_trajectory(policy_iteration_method(epsilon))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
